- Man nennt **Selbstorganisierende Karten** auch **Kohonen**
- Diese **Netze** sind **Beispiele für unüberwachtes Lernen**
	- Genauer: des **konkurrierenden Lernens** (competitive learning).
- Es **gibt Eingabe-** und **Ausgabeneuronen**. Zu **jedem Zeitpunkt feuert** nur **eine Ausgabe** (winner takes all Prinzip).
- **Ähnliche Eingaben** sollen auf die **gleiche Kategorie** **abgebildet** werden und das **selbe Neuron** soll **feuern**.
- Dabei **findet** das **Netz** die **Kategorien selbstständig**, **aufgrund der Eingabedaten**
- **Anwendung** ist z.B. die **Datenkompression**
	- **Abspeichern** von **Bildern** in die **Indexe** der **Referenzvektoren**.

### Vorstufe: Vektorquantisierung

- **Eingabe**: Vektor $(x_1,x_2,\ldots,x_n) \in \mathbb{R}^n$
- **Referenzvektoren**: $(\underline{r}_1, \underline{r}_2, \ldots, \underline{r}_n) \in \mathbb{R}^n$
[Grafik]

- **Jedem Referenzvektor** kann **zusätzlich** ein **räumlicher Bezug** gegeben werden, indem er auf der **Ebene** oder einer **Linie platziert wird** 
- Dabei soll **folgendes Prinzip gelten**:
	- **benachbarte Ausgabeneuronen entsprächen ähnlichen Eingabemustern.** **Merkmalskarte** (Feature Map)

### Prinzip der Merkmalskarte

- Zwei **Eingabevektoren** $\underline{x}^1, \underline{x}^2$. Dazu gibt es ein **feuerndes Neuron** $i$ und $j$. In der **Ebene** haben diese **Vektoren** $\underline{r}^i=(r^i_1, r^i_2)$ und $\underline{r}^j=(r^j_1,r^j_2)$. **Je ähnlicher** die **Eingaben** $\underline{x}^1$ und $\underline{x}^2$ sind, **desto kleiner** ist der **Abstand** von $\underline{r}^i$ zu $\underline{r}^j$.