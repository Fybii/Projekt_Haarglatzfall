- *Auch **Autoassoziativspeicher** 
	**Problemstellung:**
	- *Gegeben sind N Muster als **Vektoren** $\underline{q}^1, \underline{q}^2,\ldots,\underline{q}^n$, die im **Netz gespeichert** werden sollen.

|  |  |  |  |  |  |
| ---- | ---- | ---- | ---- | ---- | ---- |
|  |  | 1 | 1 | 1 |  |
|  |  |  | 1 |  |  |
|  |  |  | 1 |  |  |
|  |  |  | 1 |  |  |
|  |  |  |  |  |  |
|  |  |  |  |  |  |
- 1 - Schwarze Pixel (1)
- ("") - Weiße Pixel (-1)

- *Zeilenweise Schreibweise*
- $(-1,-1,1,1,1,-1,-1,-1,-1,1,-1,-1,-1,-1,-1,1 ... ,-1)$
- ***Aufgabe:** 
	- *Bei **Eingabe** eines **Musters** soll im Laufe der Zeit das **gespeicherte Muster**, das am **ähnlichsten** ist, **"ausgegeben"** werden.*

- *Wird ein **neues Muster eingesetzt**, so soll **jenes Muster** $q^k$ **ausgegeben** werden, das **der Eingabe** am **nächsten** liegt.

- **Klassische Anwendung ist die Schrifterkennung**

## Das Modell

- *wurde **1982 von Hopfield** vorgestellt.
- *Als **Zustände** werden **-1 und +1** verwendet. 
- *Die **Zustände** sind mit $S_j \in \{-1,1\}$ bezeichnet.
	Die **dynamische Gleichung**:
$$
S_i(t+1)=sgn \left(\sum^{}_{j} w_{ij}·S_j(t)\right)
$$
$$
sgn(x)= \left\{ \begin{array}{cc} 1 & \ falls  \ x \geq 0 \\
								 -1 & falls \ x < 0\end{array}\right.
$$
- ***Nebenbedingung:*** $w_{ij}=w_{ji}, w_{ij}=0$

$$
w_{ij}= \frac{1}{n} \sum^{N}_{k=1} p^k_i · p^k_j
$$
## Wie muss das lernen aussehen?

- ***1. Für ein Muster, $N=1:$
	- $S_i(t+1)=sgn \left(\sum^{}_{j} w_{ij}·S_j(t)\right)$
	- *für den **Fixpunkt  (konstant) gilt** dann:
		- $q_i=sgn(\sum^{}_{j} w_{ij} q_j)$
			- *setze $w_{ij}=c·q_i·q_j,\; \; c<0:$
				- $$\Rightarrow q_i=sgn \left(c·\sum^{}_{j}q^2_j·q_i\right)=sgn\left(c·q_i\sum^{}_{j}q^2_j\right)\; mit\; c>0 \wedge \left(\sum^{}_{j}·q^2_j\right) \geq 0$$
- ***2. Für N Muster:***
	- $$w_{ij}=\frac{1}{N}\sum^{N}_{k=1}p^k_ip^2_j$$
- ***Frage ist:***
	- *wie viele **Muster** können gelernt werden?*
		- *die **maximale Anzahl** an **Mustern** ist:
			- $N_{max}=0,146·n \; mit \ n:Anzahl\;der\;Neuronen$
