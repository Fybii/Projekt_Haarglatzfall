- *Da es **keine Rückkopplungen gibt**, muss die **Eingabe nur einmal durchlaufen**, bis ein **Endzustand erreicht wird**.
	*Aus der **dynamischen Gleichung** folgt:
$$

y_i=g\left(\sum^{}_{j}w_{ij}x_j-v_i\right)

$$
- $x_j:$ **Zustand des $j$-ten Eingabeneurons
- $y_i:$ **Zustand des $i$-ten Eingabeneurons 
- $g:$ **Aktivierungsfunktion**
- $v_i:$ **Schwellwert**
- $w_{ij}:$ **Kopplung zwischen Neuron $j$ und $i$

### Aufgabe:

- ***Gegeben** sind **Paare** von **Eingabemustern** $\underline{q}^k$ mit zugehörigem **Ausgabemuster (Zielmuster)** $\underline{p}^k$
	***Gesucht sind Gewichtungen*** $w_{ij}$ *und **Schwellwerte** $v_i$, sodass bei **Eingabe $q^k$ ins Netz** die **Ausgabe** $q^k$ ist für **alle Muster $k$**.

- ***Es soll Gelten:***
$$
p^k_i=g\left(\sum^{}_{j}w_{ij}q^k_j-v_i\right)
$$
- ***für*** $k=1,2,3,\ldots$

## Perzeptren als Klassifizierer

- *Dafür sind für die **Aktivierungsfunktion** die **Vorzeichenfunktion** $sgn(x)$ verwendet.

- ***Es muss gelten: $p^k_i=sgn\left(\sum^{}_{j}w_{ij}q^k_j-v_i\right)$
	$\Rightarrow p^k_i \in \{-1,1\}$ *d. h. die Zielmuster müssen binär sein*

- ***Beobachtung:** Die **Ausgabeneuronen sind unabhängig**, daher kann **ein einzelnes herausgegriffen werden**.
	$\Rightarrow$ ***Index $i$ kann weggelassen werden**
		$\Rightarrow$ aus $w_{ij}$ wird ein **Vektor** $\underline{w}=(w_1,\ldots,w_n)$ und es gibt nur **einen Schwellwert $v$***.

- *Aus **obiger Gleichung** wird dann 
$$
p^k=sgn(\underline{w}·\underline{q}^k-v) \ \ \ \ \ \ \ \ \ \underline{w}·\underline{q}^k=\sum^{}_{j}w_iq^k_i
$$
- ***für alle Muster $k=1,2,\ldots$
- *Also **muss** das **Vorzeichen** von $\underline{w}·\underline{q}^k-v$ **gleich dem Vorzeichen von $p^k$ sein.**
	***oder anders ausgedrückt:*** $\underline{w}·\underline{q}^k \geq v$ **gilt genau dann, wenn $p^k \geq 0$ ist.

- $\underline{w}·\underline{x}=v$ *ist eine **Ebene senkrecht** zu $\underline{w}$ mit **Abstand** $\frac{|v|}{||w||}$ vom **Ursprung

- *Damit der **Klassifizierer korrekt arbeitet**, muss die **Hyperebene** die **positiven von den negativen Zielwerten trennen**.
- *Ansonsten kann das **Perzeptron** die **Aufgabe nicht lösen*.
- *Das **Problem** muss **linear separabel** sein.

## Beispiele

1. ***Die Logische UND-Funktion
	
| $q_1$ | $q_2$ | $q_1 \wedge q_2=p$ |
| ---: | ---: | :--: |
| $-1$ | $-1$ | $-1$ |
| $-1$ | $1$ | $-1$ |
| $1$ | $-1$ | $-1$ |
| $1$ | $1$ | $+1$ |
- *Das **Problem** ist **linear seperabel**

2. ***Die logische Exklusiv ODER-Funktion***
	

| $q_1$ | $q_2$ | $p$ |
| ---: | ---: | ---: |
| $-1$ | $-1$ | $-1$ |
| $-1$ | $1$ | $1$ |
| $1$ | $-1$ | $1$ |
| $1$ | $1$ | $-1$ |
- *Das **Problem** ist **nicht linear separabel**.

## Vereinfachung der Gleichungen

- *Die **Gleichung:** 
$$
y_i=g\left(\sum^{}_{j}w_{ij}x_j-v_i\right)
$$
- *kann zu der **Gleichung:
$$
y_i=g\left(\sum^{}_{j}w_{ij}x_j\right)
$$
	**vereinfacht werden, wenn** ein **zusätzliches Eingabeneuron** $x_0$ **eingeführt** wird mit **konstanten Wert** $x_0=1$

$$
\begin{align}
\sum^{n}_{j=0}w_{ij} \ x_j &= w_{i0} \ x_0+\sum^{n}_{j=0}w_{ij} \ x_j \\ &= \sum^{n}_{j=0}w_{ij} \ x_j-w_{i0} \\
&=v_i
\end{align}
$$
## Eine Lernregel

- ***Voraussetzung:** Das **Problem** ist **linear seperabel**.
	*Es werden **alle Eingabemuster** $\underline{q}^k$ **durchgegangen** und **alle Ausgaben $y_i$ angeschaut.**
	*Es wird **geprüft**, ob die **Ausgabe $y_i$** der **Sollausgabe** $p^k_i$ entspricht.
	**Falls nicht**, wird nach der **Hebb'schen Regel** die **Gewichte** angepasst.*
$$
\triangle w_{ij} \left\{\begin{array}{cc}
η·P^k_iq^k_j & falls \ y_i \neq p^k_i \\
& sonst \; 0
\end{array} \right. 
$$
$$η=Lernrate$$
## Das Perzeptron als linearer Regressor

- *Hier wird für die **Aktivierungsfunktion** im $g(x)=x$ **verwendet**.
	$\Rightarrow y_i=\sum^{}_{j}w_{ij} \ x_j$
- *Die **Werte** für $x_i$ und $<y_i$ können jetzt **kontinuierlich sein.***
- *Die **Bedingung** für ein **korrekt arbeitendes Netz** ist dann:*
$$
p^k_i=\sum^{}_{j}w_{ij} \ q^k_j
$$
- *Das ist eine **Matrixgleichung: $\underline{p}^k=W·\underline{q}^k$**
- *mit der **Gewichtsmatrix** $W=(w_{ij}) \Rightarrow LGS$

### Lernmethode

- *Wir stellen eine **Kostenfunktion** auf, in **Abhängigkeit der Gewichte** $w_{ij}:$
$$
E(w)=\frac{1}{2} · \sum^{}_{i,k}(p^k_i-o^k_i)^2
$$
- $o^k_i=\sum^{}_{j}w_{ij} \ q^k_j$

- ***Mathematisch** verwendet man einen **Gradientenabstieg** für die **Anpassung der Gewichte.**

$$
\triangle w_{ij} = -η \ \frac{\delta E}{\delta w_{ij}} \ mit \ η > 0
$$