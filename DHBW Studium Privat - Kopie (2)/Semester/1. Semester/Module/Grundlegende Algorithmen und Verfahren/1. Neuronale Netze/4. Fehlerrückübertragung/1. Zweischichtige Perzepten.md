**wir wissen**: **XOR** kann **nicht** mit einem **einfachen Perzeptron** realisiert werden.

## XOR
| $x_1$ | $x_2$ | $XOR$ |
| ---: | ---: | ---: |
| $-1$ | $-1$ | $-1$ |
| $-1$ | $1$ | $1$ |
| $1$ | $-1$ | $1$ |
| $1$ | $1$ | $-1$ |

---
- Durch **Einführen** von nur **einer** **Zwischenschicht**, können **alle logischen Funktionen** realisiert werden.

- Eine passende **Lernregel** hat sich erst in den **1980er Jahren** etabliert. Die **Methode** heißt **Fehlerrückübertragung** (Backpropagation).
	$\rightarrow$ Momentan das **meistgenutzte Lernverfahren**.
	$\rightarrow$ Es ist **universell einsetzbar**.
	$\rightarrow$ **Funktioniert nur** bei **vorwärtsgetriebenen Netzwerken**.

- Wir betrachten hier **nur ein zweischichtiges Netz**:
[Grafik]

### Speziell für XOR:

![[Pasted image 20240206103629.png]]
- $V_1=-1$
- $v_1=v_2=1$

- #### Aktivierungsfunktion: 
$$
\begin{align}
sgn(x)=\left\{
\begin{array}{cc}
1 \ &falls \ x \geq 0 \\
-1 \ &sonst
\end{array} 
\right.
\end{align}
$$
- #### dynamische Gleichung allgemein:
$$
\begin{align}
z_1=sgn(w_{11}x_1-w_{12}x_2-v_1) \\
z_2=sgn(w_{21}x_1+w_{22}x_2 - v_2) \\
y_1=sgn(W_{12}z_1+W_{12}z_2-V_1)
\end{align}
$$
- #### Im Fall XOR:
$$
\begin{align}
z_1=sgn(x_1-x_2-1) \\
z_2=sgn(x_2-x_1-1) \\
y_1=sgn(z_1+z_2+1) \\
\end{align}
$$

### Funktioneller Zusammenhang

| $x_1$ | $x_2$ | \| | $z_1$ | $z_2$ | \| | $y_1$ |
| ---: | ---: | :--: | ---: | ---: | :--: | ---: |
| $-1$ | $-1$ | \| | $-1$ | $-1$ | \| | $-1$ |
| $-1$ | $1$ | \| | $-1$ | $1$ | \| | $1$ |
| $1$ | $-1$ | \| | $1$ | $-1$ | \| | $1$ |
| $1$ | $1$ | \| | $-1$ | $-1$ | \| | $-1$ |
## Lernregel im Stil der Fehlerrückübertragung

$$
\begin{align}
\triangle W_{11} &= η ·(p-y_1)·z_1 & \triangle w_{11} = \eta · (p-y_1) · W_{11} · x_1 \\
\triangle W_{12} &= \eta · (p-y_1) · z_2 & \triangle w_{12} = \eta · (p-y_1) · W_{12} · x_2 \\
\triangle V_1 &= - \eta · (p-y_1) & \triangle w_{21} = \eta · (p-y_1) · W_{21} · x_1 \\
\triangle v_1 & = - \eta · (p-y_1) · W_{11} &\triangle w_{22} = \eta · (p-y_1) · W_{22} · x_2 \\
\triangle v_2 & = - \eta · (p-y_1) · W_{12}
\end{align}
$$
- $p$ ist das **Zielmuster
- $\eta > 0$ ist die **Lernrate**.

